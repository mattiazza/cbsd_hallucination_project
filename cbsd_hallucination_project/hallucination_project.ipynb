{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallucination Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by gathering all libraries and modules needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import *\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by extracting the Abstract and Reasoning Corpus (ARC) dataset, composed with 400 `.json` files in the training and another 400 in evaluation folder.\n",
    "\n",
    "With the `data_extraction` function we convert every `.json` file into a dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_dataset: 1301\n",
      "evaluation_dataset: 1363\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\n",
    "    \"/Users/mattiapiazza/Documents/University/cognitive_behavioral_and_social_data/project/cbsd_hallucination_project/ARC_master/data\"\n",
    ")\n",
    "\n",
    "# training_dataset = data_extraction(data_path=Path(data_path / \"training\"))\n",
    "training_dataset = data_extraction(data_path=data_path / \"training\")\n",
    "evaluation_dataset = data_extraction(data_path=data_path / \"evaluation\")\n",
    "\n",
    "# Small check\n",
    "print(f\"\"\"training_dataset: {len(training_dataset[\"train\"][\"input\"])}\n",
    "evaluation_dataset: {len(evaluation_dataset[\"train\"][\"input\"])}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted our data, we need to convert them in \"arrays of strings\" so we can use them as prompt for the LLMs. In the meantime we update our dataset adding the response of the model in exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array #: 8/1301\n",
      "1 -> XXXXXXXXXXXXXXX\n",
      "2 -> XXXXXXXXXXXXXXX\n",
      "3 -> RRRRRRRRRRRRRRR\n",
      "4 -> XXXXXXXXXXXXXXX\n",
      "5 -> XXXXXXXXXXXXXXX\n",
      "6 -> BBBBBBBBBBBBBBB\n",
      "7 -> XXXXXXXXXXXXXXX\n",
      "8 -> XXXXXXXXXXXXXXX\n",
      "9 -> VVVVVVVVVVVVVVV\n",
      "10 -> XXXXXXXXXXXXXXX\n",
      "11 -> XGGGGGGXXXXXXXX\n",
      "12 -> XGGGGGGXYYYYYYY\n",
      "13 -> XGGGGGGXXXXXXXX\n",
      "14 -> XGGGGGGXXXXXXXX\n",
      "15 -> XGGGGGGXCCCCCCC\n",
      "16 -> XGGGGGGXXXXXXXX\n",
      "17 -> XXXXXXXXXXXXXXX\n",
      "18 -> PPPPPPPPPPPPPPP\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in training_dataset.items():  # key: ['train', 'test']\n",
    "    for k, val in value.items():  # k: ['input', 'output']\n",
    "        temp = []\n",
    "\n",
    "        if \"GPTsays\" not in value.keys():\n",
    "            training_dataset[key][\n",
    "                \"GPTsays\"\n",
    "            ] = []  # Adding the key for ChatGPT response to input\n",
    "\n",
    "        for v in val:  # We are taking one array at a time\n",
    "            array_colored = from_num_to_col(\n",
    "                v\n",
    "            )  # Transform the array from number to colour (letter)\n",
    "            temp.append(array_colored)\n",
    "\n",
    "            print(f\"Array #: {val.index(v)+1}/{len(val)}\\n{array_colored}\\n\")\n",
    "\n",
    "            # We want to add the actual response to our dataset (just once)\n",
    "            if k == \"input\":\n",
    "                training_dataset[key][\"GPTsays\"].append(\n",
    "                    input(\"Insert here ChatGPT output: \")\n",
    "                )\n",
    "                clear_output(wait=True)  # Clear output each time\n",
    "        training_dataset[key][k] = temp\n",
    "\n",
    "        # Saving our progress\n",
    "        dir_path = Path(\n",
    "            \"/Users/mattiapiazza/Documents/University/cognitive_behavioral_and_social_data/project/cbsd_hallucination_project/data\"\n",
    "        )\n",
    "\n",
    "        with open(Path(dir_path / \"training_dataset.json\"), \"w\") as f:\n",
    "            json.dump(training_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question our LLM\n",
    "Now that we have our prompts it's time to use them to question the LLM chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
